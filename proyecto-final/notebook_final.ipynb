{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista de cosas por hacer:\n",
    "- [ ] Más análisis\n",
    "    - [x] Riqueza léxica\n",
    "    - [ ] Que palabras prefieren\n",
    "    - [ ] Longitud media de frases\n",
    "    - [ ] Longitud media de palabras\n",
    "    - [ ] De todos los textos leidos, que palabras aparecen más, cuáles menos y en que tipo son más frecuentes.\n",
    "- [ ] Dividir dataset en 60/20/20 variados\n",
    "- [ ] Entrenarlo y evaluar su capacidad de deteccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580020c4-bdd1-475b-afa6-4e3bf8eaeac5",
   "metadata": {},
   "source": [
    "Comienzo de la práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d96c9-5ae2-4d9a-bb39-7d3ec651e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd #procesado del csv pasa E/S\n",
    "import matplotlib as plt #printeo\n",
    "import random #operaciones matematicas\n",
    "from metodos import * #importamos metodos para una mayor legibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1392f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for dirname, _, filenames in os.walk('/home/ubuntu/Escritorio/repos/segundo-cuatri/NLP/NLP-practicas_grupo/proyecto-final'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d92929-910d-40c3-ad20-0ebf319a24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('/home/ubuntu/Escritorio/repos/segundo-cuatri/NLP/NLP-practicas_grupo/proyecto-final/archive/AI_Human.csv')\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e087b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_totales = (datos['generated']).count()\n",
    "datos_humanos = (datos['generated'] == 0.0).sum()\n",
    "print(\"Datos totales: \", datos_totales)\n",
    "print(\"Humanos: \", datos_humanos)\n",
    "print(\"IA: \", datos_totales - datos_humanos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(datos['text'][0])\n",
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_ia = []\n",
    "tokens_human = []\n",
    "i = (len(datos['text'])//20)\n",
    "while i > 0:\n",
    "    valor = random.randint(0, len(datos['text']))\n",
    "    token = nltk.word_tokenize(datos['text'][valor])\n",
    "    if datos['generated'][i] == 1.0:\n",
    "        tokens_ia.append(token)\n",
    "    else:\n",
    "        tokens_human.append(token)\n",
    "    i -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61848e",
   "metadata": {},
   "source": [
    "Let's find the lexical richness of the AI texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_richness_ia = 0\n",
    "num = 0\n",
    "for i in range(0, len(tokens_ia)):\n",
    "    medium_richness_ia += lexical_richness(tokens_ia[i])\n",
    "    num += 1\n",
    "medium_richness_ia = medium_richness_ia/num\n",
    "medium_richness_ia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede1bc3",
   "metadata": {},
   "source": [
    "Let's find the lexical richness of the Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_richness_human = 0\n",
    "num = 0\n",
    "for i in range(0, len(tokens_human)):\n",
    "    medium_richness_human += lexical_richness(tokens_human[i])\n",
    "    num += 1\n",
    "medium_richness_human = medium_richness_human/num\n",
    "medium_richness_human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7755cef",
   "metadata": {},
   "source": [
    "We're going to chech the performance and we're gonna display the accuracy of the tagged words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = tokens_ia\n",
    "\n",
    "def pos_tagged_words(tagged_sents):\n",
    "    tagged_words = []\n",
    "    for sent in tagged_sents:\n",
    "        for word in sent:\n",
    "            tagged_words.append(word)\n",
    "    return tagged_words\n",
    "\n",
    "tagged_words = pos_tagged_words(tagged_sents)\n",
    "tagged_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

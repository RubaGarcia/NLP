{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lista de cosas por hacer:\n",
    "- [ ] Más análisis\n",
    "    - [x] Riqueza léxica\n",
    "    - [ ] Que palabras prefieren\n",
    "    - [ ] Longitud media de frases\n",
    "    - [ ] Longitud media de palabras\n",
    "    - [ ] De todos los textos leidos, que palabras aparecen más, cuáles menos y en que tipo son más frecuentes.\n",
    "- [ ] Dividir dataset en 60/20/20 variados\n",
    "- [ ] Entrenarlo y evaluar su capacidad de deteccion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580020c4-bdd1-475b-afa6-4e3bf8eaeac5",
   "metadata": {},
   "source": [
    "Comienzo de la práctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d96c9-5ae2-4d9a-bb39-7d3ec651e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import pandas as pd #procesado del csv pasa E/S\n",
    "import matplotlib as plt #printeo\n",
    "import random #operaciones matematicas\n",
    "from metodos import * #importamos metodos para una mayor legibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1392f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for dirname, _, filenames in os.walk('/home/ubuntu/Escritorio/repos/segundo-cuatri/NLP/NLP-practicas_grupo/proyecto-final'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d92929-910d-40c3-ad20-0ebf319a24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv('/home/ubuntu/Escritorio/repos/segundo-cuatri/NLP/NLP-practicas_grupo/proyecto-final/archive/AI_Human.csv')\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e087b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_totales = (datos['generated']).count()\n",
    "datos_humanos = (datos['generated'] == 0.0).sum()\n",
    "print(\"Datos totales: \", datos_totales)\n",
    "print(\"Humanos: \", datos_humanos)\n",
    "print(\"IA: \", datos_totales - datos_humanos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d189d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = nltk.word_tokenize(datos['text'][0])\n",
    "nltk.pos_tag(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda7dce",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "487235",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/range.py:385\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mValueError\u001b[0m: 487235 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m     valor \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(datos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m----> 6\u001b[0m     token \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(\u001b[43mdatos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalor\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m datos[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m      8\u001b[0m         tokens_ia\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/series.py:942\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 942\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/series.py:1051\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/pandas/core/indexes/range.py:387\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_loc(key, method\u001b[38;5;241m=\u001b[39mmethod, tolerance\u001b[38;5;241m=\u001b[39mtolerance)\n",
      "\u001b[0;31mKeyError\u001b[0m: 487235"
     ]
    }
   ],
   "source": [
    "tokens_ia = []\n",
    "tokens_human = []\n",
    "i = (len(datos['text'])//20)\n",
    "while i > 0:\n",
    "    valor = random.randint(0, len(datos['text']))\n",
    "    token = nltk.word_tokenize(datos['text'][valor])\n",
    "    if datos['generated'][i] == 1.0:\n",
    "        tokens_ia.append(token)\n",
    "    else:\n",
    "        tokens_human.append(token)\n",
    "    i -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61848e",
   "metadata": {},
   "source": [
    "Let's find the lexical richness of the AI texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_richness_ia = 0\n",
    "num = 0\n",
    "for i in range(0, len(tokens_ia)):\n",
    "    medium_richness_ia += lexical_richness(tokens_ia[i])\n",
    "    num += 1\n",
    "medium_richness_ia = medium_richness_ia/num\n",
    "medium_richness_ia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ede1bc3",
   "metadata": {},
   "source": [
    "Let's find the lexical richness of the Human texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bfdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_richness_human = 0\n",
    "num = 0\n",
    "for i in range(0, len(tokens_human)):\n",
    "    medium_richness_human += lexical_richness(tokens_human[i])\n",
    "    num += 1\n",
    "medium_richness_human = medium_richness_human/num\n",
    "medium_richness_human"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7755cef",
   "metadata": {},
   "source": [
    "We're going to chech the performance and we're gonna display the accuracy of the tagged words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bef1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sents = tokens_ia\n",
    "\n",
    "def pos_tagged_words(tagged_sents):\n",
    "    tagged_words = []\n",
    "    for sent in tagged_sents:\n",
    "        for word in sent:\n",
    "            tagged_words.append(word)\n",
    "    return tagged_words\n",
    "\n",
    "tagged_words = pos_tagged_words(tagged_sents)\n",
    "tagged_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
